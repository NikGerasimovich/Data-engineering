# Apache Spark

Apache Spark — это высокопроизводительный фреймворк для обработки больших данных в распределенной среде. Он поддерживает широкие возможности для анализа данных, машинного обучения, потоковой обработки и работы с графами.

## Основные характеристики Apache Spark

1. **Высокая скорость**  
   Spark выполняет операции быстрее, чем традиционные MapReduce-фреймворки, благодаря использованию **in-memory computing** (вычисления в оперативной памяти).  
   Он минимизирует операции чтения/записи с диска, сохраняя промежуточные данные в памяти, что делает его в 10–100 раз быстрее.

2. **Поддержка различных типов данных**  
   Spark может обрабатывать структурированные, полуструктурированные и неструктурированные данные.  
   Он работает с данными из файлов (JSON, CSV, Avro), баз данных, потоков данных и распределенных систем, таких как HDFS и Amazon S3.

3. **Модульная архитектура**  
   Spark состоит из нескольких компонентов, которые делают его универсальным инструментом:
   - **Spark Core**
   - **Spark SQL**
   - **Spark Streaming**
   - **MLlib (Machine Learning Library)**
   - **GraphX**

---

# Компоненты Apache Spark

<image src="https://github.com/NikGerasimovich/Data-engineering/blob/main/Spark/IMAGE/spark%20components.png" alt="component">

## Spark Core
Основной компонент, отвечающий за выполнение всех вычислительных задач. Ядро отвечает за планирование, распределение и мониторинг приложений, выполняющих множество различных задач на вычислительном кластере. Примерами таких задач являются выполнение SQL запросов или машинное обучение.

## Spark SQL
Компонент Spark SQL предназначен для работы со структурированными данными. Этот компонент работает с данными, используя диалект SQL **Hive Query Language (HQL)**. При этом, Spark SQL умеет работать с множеством различных источников данных, таких как таблицы Hive, Parquet и JSON. Также Spark SQL позволяет смешивать в одном приложении запросы SQL с программными конструкциями на Python, Java и Scala, поддерживаемыми абстракцией **RDD**, и таким способом комбинировать SQL со сложной аналитикой.

### RDD (Resilient Distributed Dataset)
<image src="https://github.com/NikGerasimovich/Data-engineering/blob/main/Spark/IMAGE/RDD.png" alt="RDD">

RDD — это определенный набор объектов, разбитых на блоки **partitions**. RDD может быть представлен как в виде структурированных наборов данных, так и неструктурированных. Partitions могут храниться на разных узлах кластера. RDD отказоустойчивы и могут быть восстановлены в случае сбоя.  
RDD состоит из набора данных, которые могут быть разбиты на блоки. Блоком, или партицией, можно считать цельную логическую неизменяемую часть данных, создать которую можно в том числе посредством преобразования (**transformations**) уже существующих блоков.

## Spark Streaming
Компонент для обработки потоковых данных в Spark называется **Streaming**. Spark Streaming имеет API для управления потоками данных, соответствующий модели RDD, поддерживаемой компонентом Spark Core, что облегчает изучение самого проекта и разных приложений обработки данных, хранящихся в памяти, на диске или поступающих в режиме реального времени.  
Примерами источников таких данных могут служить файлы журналов, заполняемые действующими веб-серверами, или очереди сообщений, посылаемых пользователями веб-служб.

## MLlib
Библиотека **MLlib** представляет собой механизм машинного обучения (**Machine Learning**, ML). Данная библиотека поддерживает множество алгоритмов машинного обучения, включая такие важные алгоритмы, как алгоритмы классификации, регрессии, кластеризации и совместной фильтрации, а также функции тестирования моделей и импортирования данных. Все эти методы способны работать в масштабе кластера.

## GraphX
В Apache Spark для решения задач обработки графов используется библиотека **GraphX**. Подобно компонентам Spark Streaming и Spark SQL, GraphX дополняет Spark RDD API возможностью создания ориентированных графов с произвольными свойствами, присваиваемыми каждой вершине или ребру. Также GraphX поддерживает разнообразные операции управления графами (такие как **subgraph** и **rnapVertices**) и библиотеку обобщенных алгоритмов работы с графами.

---

## Основные компоненты архитектуры

### DRIVER
Исполнитель программы:
- Исполняет код программы
- Планирует и запускает работу **executors**

### EXECUTOR
Выполняет вычисления:
- Исполняет код от **driver**
- Передает **driver'у** информацию о процессе вычислений

### CLUSTER MANAGER
Занимается управлением реальными машинами кластера и контролирует выделение ресурсов для Spark-приложений:
- **standalone**
- **YARN**
- **Mesos**

---

# Структуры данных

## Хронология появления структур данных по версиям Spark

### RDD — начиная с Spark 0 (Low level API)
**Resilient Distributed Dataset (RDD)** — это определенный набор объектов, разбитых на блоки **partitions**. RDD может быть представлен как в виде структурированных наборов данных, так и неструктурированных. Partitions могут храниться на разных узлах кластера. RDD отказоустойчивы и могут быть восстановлены в случае сбоя.

### DataFrame — начиная с Spark 1.3 (Structured API)
**DataFrame** — это набор типизированных записей, разбитых на блоки. Иными словами, это таблица, состоящая из строк и столбцов. Блоки могут обрабатываться на разных нодах кластера. **DataFrame** может быть представлен только в виде структурированных или полуструктурированных данных. Данные представлены именованным набором столбцов, напоминая таблицы в реляционных БД.

### DataSet — начиная с Spark 1.6 (Structured API)
**DataSet** — это набор записей типа **Row**, разбитых на блоки.

---

## RDD, DataFrame и DataSet

- **RDD** состоит из набора данных, которые могут быть разбиты на блоки. Блоком, или партицией, можно считать цельную логическую неизменяемую часть данных, создать которую можно в том числе посредством преобразования (**transformations**) уже существующих блоков.
- **DataFrame** можно создать из **RDD**. После подобного **transformation** вернуться к **RDD** уже не получится. То есть исходный **RDD** не подлежит восстановлению после трансформации в **DataFrame**.
- **DataSet**: функционал Spark позволяет преобразовывать как **RDD**, так и **DataFrame** в сам **DataSet**.

---

## Источники для структур данных

### Data Sources API

- **RDD**: **Data source API** позволяет **RDD** формироваться из любых источников, включая текстовые файлы, при этом необязательно даже структурированных.
- **DataFrame**: **Data source API** позволяет обрабатывать разные форматы файлов (AVRO, CSV, JSON, а также из систем хранения HDFS, **HIVE** таблиц, MySQL).
- **DataSet**: **Dataset API** также поддерживает различные форматы данных. **Spark DataFrame** может быть создан из различных источников.

<image src="https://github.com/NikGerasimovich/Data-engineering/blob/main/Spark/IMAGE/spark%20data%20source.png" alt="sopurce">

---

# Lazy evaluation

Отложенные (ленивые) вычисления реализуются с **RDD**, **DataFrame** и **DataSet** похожим образом — в результате выполнения действия (вычисление, возвращающее результат, подразумевающий обмен данных между **executors** и **driver**).  
При работе с **RDD** результат вычисляется не в момент определения — вместо этого формируется структурная последовательность преобразований, которая была реализована над начальным **RDD**. При этом само преобразование будет реализовано, когда необходимо отобразить или передать итог преобразований. В **DataFrame** и **DataSet** вычисления происходят похожим образом: в момент, когда требуется некое действие над ним (например, **show()**, **count()**, **collect()**, **saveAs()** и т. п.).

<image src="https://github.com/NikGerasimovich/Data-engineering/blob/main/Spark/IMAGE/lazy%20evaluation.jpg" alt="Lazyevaluation">

---

# Преимущества Apache Spark

- **Масштабируемость**: Spark может работать на одном компьютере или в распределенном кластере с тысячами узлов.
- **Совместимость**: Легко интегрируется с Hadoop, Cassandra, HBase, Hive и другими платформами.
- **Интерактивность**: Поддерживает интерактивную работу через интерфейсы, такие как PySpark или Jupyter Notebooks.
- **Многоязычная поддержка**: Поддерживает Java, Scala, Python и R.

---

# Примеры использования

## Анализ данных
- Обработка больших объемов данных для бизнес-аналитики.
- Генерация отчетов.
## Машинное обучение
## Потоковая обработка
- Реализация систем реального времени, таких как мониторинг сети или анализ данных из сенсоров IoT.
## Обработка графов
- Социальные сети, анализ связей между пользователями, нахождение рекомендаций.
